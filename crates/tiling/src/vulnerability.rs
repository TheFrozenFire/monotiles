//! Recoverability vulnerability analysis for tiling deflation.
//!
//! Maps the recoverability landscape: per-position criticality, minimum
//! determining sets, erasure fuzzing, and dependency chains. Built on the
//! one-way analysis infrastructure. Generic over any TilingSystem.

use std::collections::{BTreeMap, HashMap, HashSet};

use tracing::info;

use crate::oneway::{
    build_hierarchy, full_sibling_adjacency, neighborhood, type_signature, FlatHierarchy,
    TypeSignature,
};
use crate::systems::hat::HatSystem;
use crate::systems::TilingSystem;

// ---------------------------------------------------------------------------
// Swap vulnerability
// ---------------------------------------------------------------------------

/// A pair of supertile types whose compositions differ by exactly one tile.
#[derive(Clone, Debug, PartialEq, Eq)]
pub struct SwapRecord {
    pub source_supertile: usize,
    pub target_supertile: usize,
    pub differing_type: usize,
    pub source_size: usize,
}

/// For each ordered pair of supertile types, check if their compositions
/// differ by exactly one tile in exactly one type component.
pub fn enumerate_valid_swaps(system: &dyn TilingSystem) -> Vec<SwapRecord> {
    let comp = system.composition();
    let num_types = system.num_types();
    let mut swaps = Vec::new();

    for src_idx in 0..num_types {
        let src_comp = &comp[src_idx];
        let src_total: usize = src_comp.iter().sum();

        for tgt_idx in 0..num_types {
            if tgt_idx == src_idx {
                continue;
            }
            let tgt_comp = &comp[tgt_idx];

            // Check if compositions differ by exactly +1 in one component, 0 in rest
            let mut diff_idx = None;
            let mut valid = true;
            for j in 0..num_types {
                let d = tgt_comp[j] as i64 - src_comp[j] as i64;
                match d {
                    0 => {}
                    1 => {
                        if diff_idx.is_some() {
                            valid = false;
                            break;
                        }
                        diff_idx = Some(j);
                    }
                    _ => {
                        valid = false;
                        break;
                    }
                }
            }

            if valid {
                if let Some(di) = diff_idx {
                    swaps.push(SwapRecord {
                        source_supertile: src_idx,
                        target_supertile: tgt_idx,
                        differing_type: di,
                        source_size: src_total,
                    });
                }
            }
        }
    }

    swaps
}

// ---------------------------------------------------------------------------
// Per-position criticality
// ---------------------------------------------------------------------------

/// Criticality of a single child position within a supertile.
#[derive(Clone, Debug)]
pub struct PositionCriticality {
    pub supertile: usize,
    pub position: usize,
    pub child_type: usize,
    /// 0 = redundant, 1 = critical (supertile becomes ambiguous without it).
    pub score: u8,
    pub confused_with: Vec<usize>,
}

/// For each supertile type and child position, compute criticality.
pub fn compute_position_criticality(system: &dyn TilingSystem) -> Vec<PositionCriticality> {
    let comp = system.composition();
    let num_types = system.num_types();
    let mut results = Vec::new();

    for st_idx in 0..num_types {
        let children = system.supertile_children(st_idx);

        for (pos, &child_rule_idx) in children.iter().enumerate() {
            let child_type = system.inflation_child_type(child_rule_idx);

            // Mask this position: remove one tile of this type from the composition
            let mut partial = comp[st_idx].clone();
            partial[child_type] -= 1;

            // Check which other supertile types could match with ANY single tile added
            let mut confused_with = Vec::new();
            for other_idx in 0..num_types {
                if other_idx == st_idx {
                    continue;
                }
                let other_comp = &comp[other_idx];
                let mut diff_sum = 0i64;
                let mut any_negative = false;
                for j in 0..num_types {
                    let d = other_comp[j] as i64 - partial[j] as i64;
                    if d < 0 {
                        any_negative = true;
                        break;
                    }
                    diff_sum += d;
                }
                if !any_negative && diff_sum == 1 {
                    confused_with.push(other_idx);
                }
            }

            let score = if confused_with.is_empty() { 0 } else { 1 };

            results.push(PositionCriticality {
                supertile: st_idx,
                position: pos,
                child_type,
                score,
                confused_with,
            });
        }
    }

    results
}

// ---------------------------------------------------------------------------
// Minimum determining sets
// ---------------------------------------------------------------------------

/// A minimal subset of child positions that uniquely identifies a supertile.
#[derive(Clone, Debug)]
pub struct DeterminingSet {
    pub supertile: usize,
    pub positions: Vec<usize>,
    pub size: usize,
}

/// For each supertile type, find the smallest subset of child positions whose
/// types uniquely identify the supertile among all types.
pub fn minimum_determining_sets(system: &dyn TilingSystem) -> Vec<DeterminingSet> {
    let num_types = system.num_types();
    let mut results = Vec::new();

    // Build type lists for all supertile types
    let all_child_types: Vec<Vec<usize>> = (0..num_types)
        .map(|si| {
            system
                .supertile_children(si)
                .iter()
                .map(|&idx| system.inflation_child_type(idx))
                .collect()
        })
        .collect();

    for st_idx in 0..num_types {
        let child_types = &all_child_types[st_idx];
        let n = child_types.len();

        // Brute-force: try subsets of increasing size
        let mut found = false;
        for size in 0..=n {
            if found {
                break;
            }

            let mut best_positions = Vec::new();
            for mask in 0u32..(1u32 << n) {
                if mask.count_ones() as usize != size {
                    continue;
                }

                let positions: Vec<usize> =
                    (0..n).filter(|&i| mask & (1 << i) != 0).collect();

                // Compute the type-count signature at these positions
                let mut sig = vec![0usize; num_types];
                for &p in &positions {
                    sig[child_types[p]] += 1;
                }

                // Check if any other supertile type could produce the same signature
                let mut unique = true;
                for (other_idx, other_children) in all_child_types.iter().enumerate() {
                    if other_idx == st_idx {
                        continue;
                    }
                    if can_match_signature(other_children, &sig, size, num_types) {
                        unique = false;
                        break;
                    }
                }

                if unique {
                    best_positions = positions;
                    found = true;
                    break;
                }
            }

            if found {
                results.push(DeterminingSet {
                    supertile: st_idx,
                    positions: best_positions,
                    size,
                });
            }
        }

        if !found {
            results.push(DeterminingSet {
                supertile: st_idx,
                positions: (0..n).collect(),
                size: n,
            });
        }
    }

    results
}

/// Check if we can pick `size` elements from `types` whose type-count
/// signature matches `target_sig`.
fn can_match_signature(
    types: &[usize],
    target_sig: &[usize],
    size: usize,
    num_types: usize,
) -> bool {
    if types.len() < size {
        return false;
    }

    let mut available = vec![0usize; num_types];
    for &t in types {
        available[t] += 1;
    }

    let mut total_needed = 0usize;
    for i in 0..num_types {
        if target_sig[i] > available[i] {
            return false;
        }
        total_needed += target_sig[i];
    }

    total_needed == size
}

// ---------------------------------------------------------------------------
// Erasure fuzzing
// ---------------------------------------------------------------------------

/// Result of an erasure trial at a given fraction.
#[derive(Clone, Debug)]
pub struct ErasureResult {
    pub fraction: f64,
    pub trials: usize,
    pub mean_recoverable: f64,
    pub std_recoverable: f64,
    pub mean_determined: f64,
}

fn xorshift64(state: &mut u64) -> u64 {
    let mut x = *state;
    x ^= x << 13;
    x ^= x >> 7;
    x ^= x << 17;
    *state = x;
    x
}

/// Erase a fraction of tile types and measure how many remaining tiles
/// can still be uniquely assigned to their parent supertile.
pub fn erasure_sweep(
    hierarchy: &FlatHierarchy,
    level: usize,
    adjacency: &[Vec<usize>],
    fractions: &[f64],
    trials_per_fraction: usize,
) -> Vec<ErasureResult> {
    let n = hierarchy.tile_types[level].len();
    let mut results = Vec::new();

    for &frac in fractions {
        let mut recoverable_rates = Vec::with_capacity(trials_per_fraction);
        let mut determined_rates = Vec::with_capacity(trials_per_fraction);

        for trial in 0..trials_per_fraction {
            let mut rng_state =
                (frac.to_bits() ^ (trial as u64).wrapping_mul(0x9E3779B97F4A7C15)) | 1;

            let erase_count = (n as f64 * frac).round() as usize;
            let mut erased = vec![false; n];

            if erase_count >= n {
                erased.iter_mut().for_each(|e| *e = true);
            } else {
                let mut indices: Vec<usize> = (0..n).collect();
                for i in 0..erase_count {
                    let j = i + (xorshift64(&mut rng_state) as usize % (n - i));
                    indices.swap(i, j);
                }
                for &idx in &indices[..erase_count] {
                    erased[idx] = true;
                }
            }

            // Group non-erased tiles by masked signature to check determination
            let mut sig_groups: HashMap<(usize, Vec<(usize, Vec<usize>)>), HashSet<usize>> =
                HashMap::new();
            let mut tile_sigs: Vec<Option<(usize, Vec<(usize, Vec<usize>)>)>> = vec![None; n];

            for tile in 0..n {
                if erased[tile] {
                    continue;
                }
                let own_type = hierarchy.tile_types[level][tile];
                let sig = masked_type_signature(hierarchy, adjacency, level, tile, 1, &erased);
                let parent = hierarchy.parent_of[level][tile];
                let parent_type = hierarchy.tile_types[level + 1][parent];
                let key = (own_type, sig);
                sig_groups.entry(key.clone()).or_default().insert(parent_type);
                tile_sigs[tile] = Some(key);
            }

            let mut determined = 0usize;
            for tile in 0..n {
                if erased[tile] {
                    continue;
                }
                if let Some(ref key) = tile_sigs[tile] {
                    if sig_groups[key].len() == 1 {
                        determined += 1;
                    }
                }
            }

            let surviving = n - erase_count.min(n);
            let rec_rate = if n > 0 {
                surviving as f64 / n as f64
            } else {
                1.0
            };
            let det_rate = if surviving > 0 {
                determined as f64 / surviving as f64
            } else {
                0.0
            };

            recoverable_rates.push(rec_rate);
            determined_rates.push(det_rate);
        }

        let mean_rec = mean(&recoverable_rates);
        let std_rec = std_dev(&recoverable_rates);
        let mean_det = mean(&determined_rates);

        results.push(ErasureResult {
            fraction: frac,
            trials: trials_per_fraction,
            mean_recoverable: mean_rec,
            std_recoverable: std_rec,
            mean_determined: mean_det,
        });
    }

    results
}

/// Type signature that ignores erased tiles.
fn masked_type_signature(
    hierarchy: &FlatHierarchy,
    adjacency: &[Vec<usize>],
    level: usize,
    tile: usize,
    radius: usize,
    erased: &[bool],
) -> Vec<(usize, Vec<usize>)> {
    use std::collections::VecDeque;

    let num_types = hierarchy.num_types;
    let mut visited: HashMap<usize, usize> = HashMap::new();
    let mut queue = VecDeque::new();
    queue.push_back((tile, 0usize));
    visited.insert(tile, 0);

    while let Some((current, dist)) = queue.pop_front() {
        if dist >= radius {
            continue;
        }
        for &neighbor in &adjacency[current] {
            if !visited.contains_key(&neighbor) {
                visited.insert(neighbor, dist + 1);
                queue.push_back((neighbor, dist + 1));
            }
        }
    }

    let mut by_distance: BTreeMap<usize, Vec<usize>> = BTreeMap::new();
    for (&tile_idx, &dist) in &visited {
        if erased[tile_idx] {
            continue;
        }
        let type_idx = hierarchy.tile_types[level][tile_idx];
        let entry = by_distance
            .entry(dist)
            .or_insert_with(|| vec![0; num_types]);
        entry[type_idx] += 1;
    }

    by_distance.into_iter().collect()
}

fn mean(values: &[f64]) -> f64 {
    if values.is_empty() {
        return 0.0;
    }
    values.iter().sum::<f64>() / values.len() as f64
}

fn std_dev(values: &[f64]) -> f64 {
    if values.len() < 2 {
        return 0.0;
    }
    let m = mean(values);
    let variance = values.iter().map(|v| (v - m).powi(2)).sum::<f64>() / (values.len() - 1) as f64;
    variance.sqrt()
}

// ---------------------------------------------------------------------------
// Dependency graph
// ---------------------------------------------------------------------------

/// Build a directed dependency graph: tile A depends on tile B if erasing B
/// changes A's determination status from determined to ambiguous.
pub fn build_dependency_graph(
    hierarchy: &FlatHierarchy,
    level: usize,
    adjacency: &[Vec<usize>],
    radius: usize,
) -> Vec<Vec<usize>> {
    let n = hierarchy.tile_types[level].len();

    let baseline_sigs: Vec<TypeSignature> = (0..n)
        .map(|i| type_signature(hierarchy, adjacency, level, i, radius))
        .collect();

    let mut sig_to_parent_types: HashMap<&TypeSignature, HashSet<usize>> = HashMap::new();
    for (i, sig) in baseline_sigs.iter().enumerate() {
        let parent = hierarchy.parent_of[level][i];
        let parent_type = hierarchy.tile_types[level + 1][parent];
        sig_to_parent_types.entry(sig).or_default().insert(parent_type);
    }

    let baseline_determined: Vec<bool> = baseline_sigs
        .iter()
        .map(|sig| sig_to_parent_types[sig].len() == 1)
        .collect();

    let mut deps: Vec<Vec<usize>> = vec![Vec::new(); n];

    for a in 0..n {
        if !baseline_determined[a] {
            continue;
        }

        let hood = neighborhood(adjacency, a, radius);
        for &b in &hood {
            if b == a {
                continue;
            }

            let mut erased = vec![false; n];
            erased[b] = true;

            let sig_a = masked_type_signature(hierarchy, adjacency, level, a, radius, &erased);

            let own_type = hierarchy.tile_types[level][a];
            let parent_a = hierarchy.parent_of[level][a];
            let parent_type_a = hierarchy.tile_types[level + 1][parent_a];

            let mut ambiguous = false;
            for c in 0..n {
                if c == a || erased[c] {
                    continue;
                }
                if hierarchy.tile_types[level][c] != own_type {
                    continue;
                }
                let parent_c = hierarchy.parent_of[level][c];
                let parent_type_c = hierarchy.tile_types[level + 1][parent_c];
                if parent_type_c == parent_type_a {
                    continue;
                }

                let sig_c = masked_type_signature(hierarchy, adjacency, level, c, radius, &erased);
                if sig_c == sig_a {
                    ambiguous = true;
                    break;
                }
            }

            if ambiguous {
                deps[a].push(b);
            }
        }
    }

    deps
}

/// Summary of dependency cascade analysis.
#[derive(Clone, Debug)]
pub struct CascadeReport {
    pub max_chain_length: usize,
    pub max_in_degree_tile: Option<usize>,
    pub max_in_degree: usize,
    pub in_degree_distribution: BTreeMap<usize, usize>,
    pub has_cycles: bool,
    pub total_edges: usize,
}

pub fn cascade_analysis(deps: &[Vec<usize>]) -> CascadeReport {
    let n = deps.len();

    let mut in_degree = vec![0usize; n];
    let mut total_edges = 0usize;
    for neighbors in deps {
        for &target in neighbors {
            in_degree[target] += 1;
            total_edges += 1;
        }
    }

    let max_in_degree_tile = in_degree
        .iter()
        .enumerate()
        .max_by_key(|(_, &d)| d)
        .filter(|(_, &d)| d > 0)
        .map(|(i, _)| i);
    let max_in_degree = in_degree.iter().copied().max().unwrap_or(0);

    let mut in_degree_distribution = BTreeMap::new();
    for &d in &in_degree {
        *in_degree_distribution.entry(d).or_insert(0usize) += 1;
    }

    let has_cycles = detect_cycles(deps);
    let max_chain_length = longest_path(deps);

    CascadeReport {
        max_chain_length,
        max_in_degree_tile,
        max_in_degree,
        in_degree_distribution,
        has_cycles,
        total_edges,
    }
}

fn detect_cycles(deps: &[Vec<usize>]) -> bool {
    let n = deps.len();
    let mut state = vec![0u8; n];

    for start in 0..n {
        if state[start] != 0 {
            continue;
        }
        let mut stack = vec![(start, 0usize)];
        state[start] = 1;

        while let Some((node, idx)) = stack.last_mut() {
            if *idx < deps[*node].len() {
                let next = deps[*node][*idx];
                *idx += 1;
                match state[next] {
                    0 => {
                        state[next] = 1;
                        stack.push((next, 0));
                    }
                    1 => return true,
                    _ => {}
                }
            } else {
                state[*node] = 2;
                stack.pop();
            }
        }
    }

    false
}

fn longest_path(deps: &[Vec<usize>]) -> usize {
    let n = deps.len();
    let mut memo = vec![None::<usize>; n];
    let mut visiting = vec![false; n];
    let mut max_len = 0usize;

    for start in 0..n {
        let len = longest_path_from(deps, start, &mut memo, &mut visiting);
        max_len = max_len.max(len);
    }

    max_len
}

fn longest_path_from(
    deps: &[Vec<usize>],
    node: usize,
    memo: &mut [Option<usize>],
    visiting: &mut [bool],
) -> usize {
    if let Some(cached) = memo[node] {
        return cached;
    }
    if visiting[node] {
        return 0;
    }

    visiting[node] = true;
    let mut max_child = 0usize;
    for &next in &deps[node] {
        let child_len = longest_path_from(deps, next, memo, visiting);
        max_child = max_child.max(child_len + 1);
    }
    visiting[node] = false;
    memo[node] = Some(max_child);
    max_child
}

// ---------------------------------------------------------------------------
// Full vulnerability analysis
// ---------------------------------------------------------------------------

/// Complete vulnerability analysis results.
pub struct VulnerabilityAnalysis {
    pub swaps: Vec<SwapRecord>,
    pub criticality: Vec<PositionCriticality>,
    pub determining_sets: Vec<DeterminingSet>,
    pub erasure_results: Vec<ErasureResult>,
    pub cascade: CascadeReport,
    pub depth: usize,
    pub base_tiles: usize,
}

/// Run the full vulnerability analysis for a given tiling system.
pub fn analyze_system(
    system: &dyn TilingSystem,
    seed: usize,
    depth: usize,
    erasure_trials: usize,
) -> VulnerabilityAnalysis {
    let swaps = enumerate_valid_swaps(system);
    let criticality = compute_position_criticality(system);
    let determining_sets = minimum_determining_sets(system);

    let hierarchy = build_hierarchy(system, seed, depth);
    let base_tiles = hierarchy.tile_types[0].len();
    let adjacency = full_sibling_adjacency(&hierarchy, 0);

    let fractions: Vec<f64> = (0..=10).map(|i| i as f64 / 10.0).collect();
    let erasure_results = erasure_sweep(&hierarchy, 0, &adjacency, &fractions, erasure_trials);

    let deps = build_dependency_graph(&hierarchy, 0, &adjacency, 1);
    let cascade = cascade_analysis(&deps);

    VulnerabilityAnalysis {
        swaps,
        criticality,
        determining_sets,
        erasure_results,
        cascade,
        depth,
        base_tiles,
    }
}

/// Hat-specific analyze (backward compatible).
pub fn analyze(depth: usize, erasure_trials: usize) -> VulnerabilityAnalysis {
    let system = HatSystem::new();
    analyze_system(&system, 0, depth, erasure_trials)
}

/// Print the vulnerability analysis report.
pub fn print_report(system: &dyn TilingSystem, analysis: &VulnerabilityAnalysis) {
    info!("=== Recoverability Vulnerability Analysis ===\n");

    // Swap enumeration
    info!("--- Valid single-tile swaps ---");
    if analysis.swaps.is_empty() {
        info!("  No valid single-tile swaps between any supertile pair.");
    } else {
        info!("  Found {} valid swap(s):", analysis.swaps.len());
        for swap in &analysis.swaps {
            info!(
                "    {} ({} children) + 1 {} = {} ({} children)",
                system.supertile_name(swap.source_supertile),
                swap.source_size,
                system.type_name(swap.differing_type),
                system.supertile_name(swap.target_supertile),
                swap.source_size + 1,
            );
        }
    }

    // Criticality
    info!("\n--- Per-position criticality ---");
    for st_idx in 0..system.num_types() {
        let positions: Vec<&PositionCriticality> = analysis
            .criticality
            .iter()
            .filter(|p| p.supertile == st_idx)
            .collect();
        let critical_count = positions.iter().filter(|p| p.score == 1).count();
        let redundant_count = positions.iter().filter(|p| p.score == 0).count();
        info!(
            "  {} ({} children): {} critical, {} redundant",
            system.supertile_name(st_idx),
            positions.len(),
            critical_count,
            redundant_count
        );
        for p in &positions {
            let status = if p.score == 0 { "redundant" } else { "CRITICAL" };
            let confused = if p.confused_with.is_empty() {
                String::new()
            } else {
                let names: Vec<&str> = p
                    .confused_with
                    .iter()
                    .map(|&i| system.supertile_name(i))
                    .collect();
                format!(" (confused with {:?})", names)
            };
            info!(
                "    pos {} ({}): {}{}",
                p.position,
                system.type_name(p.child_type),
                status,
                confused
            );
        }
    }

    // Minimum determining sets
    info!("\n--- Minimum determining sets ---");
    for ds in &analysis.determining_sets {
        info!(
            "  {}: size {} (positions {:?})",
            system.supertile_name(ds.supertile),
            ds.size,
            ds.positions
        );
    }

    // Erasure sweep
    info!(
        "\n--- Erasure sweep (depth={}, {} base tiles) ---",
        analysis.depth, analysis.base_tiles
    );
    info!(
        "  {:>8} {:>12} {:>12}",
        "Fraction", "Surviving", "Determined"
    );
    for er in &analysis.erasure_results {
        info!(
            "  {:>8.1}% {:>11.1}% {:>11.1}%",
            er.fraction * 100.0,
            er.mean_recoverable * 100.0,
            er.mean_determined * 100.0,
        );
    }

    // Dependency graph
    info!("\n--- Dependency graph (radius 1) ---");
    info!("  Total edges: {}", analysis.cascade.total_edges);
    info!("  Has cycles: {}", analysis.cascade.has_cycles);
    info!(
        "  Max chain length: {}",
        analysis.cascade.max_chain_length
    );
    if let Some(tile) = analysis.cascade.max_in_degree_tile {
        info!(
            "  Most depended-upon tile: {} (in-degree {})",
            tile, analysis.cascade.max_in_degree
        );
    }
    info!("  In-degree distribution:");
    for (&degree, &count) in &analysis.cascade.in_degree_distribution {
        info!("    degree {}: {} tiles", degree, count);
    }

    // Summary
    info!("\n=== SUMMARY ===");
    let swap_count = analysis.swaps.len();
    let unique_swap_pairs: HashSet<(usize, usize)> = analysis
        .swaps
        .iter()
        .map(|s| {
            if s.source_supertile < s.target_supertile {
                (s.source_supertile, s.target_supertile)
            } else {
                (s.target_supertile, s.source_supertile)
            }
        })
        .collect();

    if swap_count > 0 {
        info!(
            "Vulnerable swap pair(s): {} ({} directional swaps between {} pair(s))",
            unique_swap_pairs
                .iter()
                .map(|(a, b)| format!(
                    "{}<->{}",
                    system.supertile_name(*a),
                    system.supertile_name(*b)
                ))
                .collect::<Vec<_>>()
                .join(", "),
            swap_count,
            unique_swap_pairs.len(),
        );
    }

    let total_critical: usize = analysis
        .criticality
        .iter()
        .filter(|p| p.score == 1)
        .count();
    let total_positions: usize = analysis.criticality.len();
    info!(
        "Critical positions: {}/{} ({:.1}%)",
        total_critical,
        total_positions,
        total_critical as f64 / total_positions as f64 * 100.0
    );

    if let Some(transition) = analysis
        .erasure_results
        .windows(2)
        .find(|w| w[0].mean_determined > 0.5 && w[1].mean_determined <= 0.5)
    {
        info!(
            "Erasure phase transition: ~{:.0}%-{:.0}% erasure",
            transition[0].fraction * 100.0,
            transition[1].fraction * 100.0,
        );
    }
}

// ---------------------------------------------------------------------------
// Modification distance analysis
// ---------------------------------------------------------------------------

/// A sibling pair of supertiles at some level that forms a confusable pair.
///
/// Moving 1 tile of `differing_type` from supertile B (the larger type) to
/// supertile A (the smaller type) swaps their labels with zero cascade cost.
#[derive(Clone, Debug)]
pub struct ModificationInstance {
    /// Hierarchy level where the supertile pair lives.
    pub supertile_level: usize,
    /// Index of supertile A (the "source" type, smaller composition).
    pub supertile_a: usize,
    /// Type label of supertile A before the swap.
    pub type_a: usize,
    /// Index of supertile B (the "target" type, larger composition).
    pub supertile_b: usize,
    /// Type label of supertile B before the swap.
    pub type_b: usize,
    /// Common parent index at supertile_level + 1.
    pub parent: usize,
    /// The tile type that distinguishes B from A (the tile being moved).
    pub differing_type: usize,
    /// Number of higher-level type changes forced by this swap (always 0 for sibling swaps).
    pub cascade_cost: usize,
}

/// Results of the modification distance analysis.
pub struct ModificationDistanceAnalysis {
    /// Confusable pairs at the composition level.
    pub swaps: Vec<SwapRecord>,
    /// Sibling pair instances per level (index = supertile level, 1..=depth-1).
    pub instances_per_level: Vec<Vec<ModificationInstance>>,
    /// Minimum number of parent-attribution changes to produce a different valid hierarchy.
    /// `None` if no confusable pairs exist (modification is impossible).
    pub min_distance: Option<usize>,
    /// Whether the minimum distance is achieved with zero cascade cost.
    pub cascade_free: bool,
    /// Total sibling pair instances across all levels.
    pub total_instances: usize,
}

/// Find all sibling instances of confusable pairs at each level of the hierarchy.
///
/// For each confusable pair (A-type, B-type) where B = A + 1 tile of `differing_type`,
/// scans the hierarchy for levels where an A-type and B-type supertile share the same parent.
/// Moving 1 `differing_type` tile from B to A converts A→B and B→A with zero cascade cost,
/// because the parent's child-type multiset is unchanged (symmetric swap).
pub fn analyze_modification_distance(
    system: &dyn TilingSystem,
    seed: usize,
    depth: usize,
) -> ModificationDistanceAnalysis {
    let swaps = enumerate_valid_swaps(system);

    if swaps.is_empty() || depth < 2 {
        return ModificationDistanceAnalysis {
            swaps,
            instances_per_level: vec![Vec::new(); depth.saturating_sub(1)],
            min_distance: None,
            cascade_free: true,
            total_instances: 0,
        };
    }

    let hierarchy = build_hierarchy(system, seed, depth);

    // Check each level from 1 to depth-1 (supertiles have a parent at level+1 ≤ depth).
    let mut instances_per_level: Vec<Vec<ModificationInstance>> = Vec::new();

    for supertile_level in 1..depth {
        let num_supertiles = hierarchy.tile_types[supertile_level].len();
        let mut level_instances = Vec::new();

        // Group supertiles at this level by their parent at supertile_level+1.
        let mut by_parent: HashMap<usize, Vec<usize>> = HashMap::new();
        for st_idx in 0..num_supertiles {
            // parent_of[k][i] = parent of tile i at level k+1, valid for k < depth
            let parent_idx = hierarchy.parent_of[supertile_level][st_idx];
            by_parent.entry(parent_idx).or_default().push(st_idx);
        }

        // For each sibling group, find all confusable pair instances.
        for (parent_idx, siblings) in &by_parent {
            for &a_idx in siblings {
                for &b_idx in siblings {
                    if a_idx >= b_idx {
                        continue; // avoid duplicates
                    }
                    let type_a = hierarchy.tile_types[supertile_level][a_idx];
                    let type_b = hierarchy.tile_types[supertile_level][b_idx];

                    // Check both orientations of the swap (src→tgt and tgt→src).
                    for swap in &swaps {
                        let (sa, sb, diff_type) = if swap.source_supertile == type_a
                            && swap.target_supertile == type_b
                        {
                            (a_idx, b_idx, swap.differing_type)
                        } else if swap.source_supertile == type_b
                            && swap.target_supertile == type_a
                        {
                            (b_idx, a_idx, swap.differing_type)
                        } else {
                            continue;
                        };

                        // Cascade cost is provably 0: swapping one instance of type_a with
                        // one instance of type_b in the parent's child list preserves the
                        // parent's composition multiset (it still has the same counts of
                        // type_a and type_b — they just exchanged identities).
                        level_instances.push(ModificationInstance {
                            supertile_level,
                            supertile_a: sa,
                            type_a: hierarchy.tile_types[supertile_level][sa],
                            supertile_b: sb,
                            type_b: hierarchy.tile_types[supertile_level][sb],
                            parent: *parent_idx,
                            differing_type: diff_type,
                            cascade_cost: 0,
                        });
                    }
                }
            }
        }

        instances_per_level.push(level_instances);
    }

    let total_instances: usize = instances_per_level.iter().map(|v| v.len()).sum();
    let min_distance = if total_instances > 0 { Some(1) } else { None };

    ModificationDistanceAnalysis {
        swaps,
        instances_per_level,
        min_distance,
        cascade_free: true,
        total_instances,
    }
}

/// Print the modification distance analysis report.
pub fn print_modification_report(system: &dyn TilingSystem, analysis: &ModificationDistanceAnalysis) {
    info!("=== Modification Distance Analysis ===\n");

    // Confusable pairs
    info!("--- Confusable pairs (composition-level) ---");
    if analysis.swaps.is_empty() {
        info!("  None: no pair of supertile types differs by exactly 1 child.");
        info!("  => Modification distance: INFINITE (no valid swap exists)");
        return;
    }
    for swap in &analysis.swaps {
        info!(
            "  {} ({} children) + 1 {} => {} ({} children)",
            system.supertile_name(swap.source_supertile),
            swap.source_size,
            system.type_name(swap.differing_type),
            system.supertile_name(swap.target_supertile),
            swap.source_size + 1,
        );
    }

    // Why type changes don't work
    info!("\n--- Base-tile type-change analysis ---");
    info!(
        "  Supertile types are uniquely determined by child COUNT, not just composition."
    );
    for swap in &analysis.swaps {
        info!(
            "    {} has {} children; {} has {} children — sizes differ, so no type-only change can convert between them.",
            system.supertile_name(swap.source_supertile),
            swap.source_size,
            system.supertile_name(swap.target_supertile),
            swap.source_size + 1,
        );
    }
    info!("  => Base-tile type-change distance: INFINITE\n");

    // Parent attribution analysis
    info!("--- Parent-attribution modification (moving 1 tile between siblings) ---");
    info!(
        "  Total sibling confusable-pair instances across all levels: {}",
        analysis.total_instances
    );

    for (level_offset, level_insts) in analysis.instances_per_level.iter().enumerate() {
        let supertile_level = level_offset + 1;
        if level_insts.is_empty() {
            info!(
                "  Level {}: 0 confusable sibling pairs",
                supertile_level
            );
        } else {
            info!(
                "  Level {}: {} confusable sibling pair(s), cascade cost = 0",
                supertile_level,
                level_insts.len()
            );
            // Show a few examples
            for inst in level_insts.iter().take(3) {
                info!(
                    "    {} #{} + {} #{}: move 1 {} tile, parent #{} composition unchanged",
                    system.supertile_name(inst.type_a),
                    inst.supertile_a,
                    system.supertile_name(inst.type_b),
                    inst.supertile_b,
                    system.type_name(inst.differing_type),
                    inst.parent,
                );
            }
            if level_insts.len() > 3 {
                info!("    ... and {} more", level_insts.len() - 3);
            }
        }
    }

    // Summary
    info!("\n=== SUMMARY ===");
    match analysis.min_distance {
        Some(d) => {
            info!("Minimum modification distance: {} (parent-attribution change)", d);
            info!("Cascade cost: 0 (symmetric sibling swap preserves parent composition)");
            info!(
                "Instances available: {} across {} levels",
                analysis.total_instances,
                analysis.instances_per_level.iter().filter(|v| !v.is_empty()).count()
            );
            info!("\nInterpretation:");
            info!(
                "  An adversary can produce a different valid hierarchy by moving 1 tile"
            );
            info!(
                "  between any sibling {} ↔ {} pair.",
                system.supertile_name(analysis.swaps[0].source_supertile),
                system.supertile_name(analysis.swaps[0].target_supertile),
            );
            info!(
                "  The dense cascade structure does NOT increase the modification distance:"
            );
            info!(
                "  the swap is local (1 tile moves between 2 siblings) with no upward propagation."
            );
        }
        None => {
            info!("Minimum modification distance: INFINITE");
            info!("No confusable sibling pairs found in the hierarchy.");
        }
    }
}

// ---------------------------------------------------------------------------
// Geometric modification distance analysis
// ---------------------------------------------------------------------------

/// A boundary tile that enables a geometric parent-attribution swap.
///
/// Boundary tiles are not assigned to any supertile. They sit at the geometric
/// junction between adjacent H'-supertiles. If such a tile has the same type as
/// the `differing_type` of a confusable pair, it can be re-attributed between
/// adjacent supertiles, enabling a geometric swap at cost 1.
#[derive(Clone, Debug)]
pub struct EnablingBoundaryTile {
    /// Child index in the flat inflation list (0..num_inflation_children).
    pub child_index: usize,
    /// Type label of this boundary tile.
    pub tile_type: usize,
    /// The confusable-pair swap this boundary tile enables.
    pub swap: SwapRecord,
}

/// Results of the geometric modification distance analysis.
pub struct GeometricModificationAnalysis {
    /// Confusable pairs at the composition level.
    pub swaps: Vec<SwapRecord>,
    /// Total number of boundary tiles in the system (tiles not assigned to any supertile).
    pub boundary_tile_count: usize,
    /// Boundary tiles whose type matches the differing_type of some confusable pair.
    pub enabling_boundary_tiles: Vec<EnablingBoundaryTile>,
    /// Minimum number of boundary-tile attribution changes to produce a different
    /// valid hierarchy. `Some(1)` if enabling tiles found; `None` means ∞.
    pub geometric_min_distance: Option<usize>,
}

/// Analyze geometric modification distance.
///
/// The combinatorial analysis shows both hat and spectre have parent-attribution
/// modification distance = 1. Geometry adds a further constraint: the tile being
/// re-attributed must be at the physical boundary between the two supertiles.
/// Only **boundary tiles** (unassigned inflation children) occupy such boundaries.
///
/// - Hat has 7 boundary children, 2 of which are F-type → P'↔F' swap is
///   geometrically possible → geometric distance = 1.
/// - Spectre has 0 boundary children → no geometric swap exists →
///   geometric distance = ∞.
pub fn analyze_geometric_modification_distance(
    system: &dyn TilingSystem,
) -> GeometricModificationAnalysis {
    let swaps = enumerate_valid_swaps(system);
    let boundary = system.boundary_children();
    let boundary_tile_count = boundary.len();

    let mut enabling = Vec::new();
    for &child_idx in boundary {
        let tile_type = system.inflation_child_type(child_idx);
        for swap in &swaps {
            if swap.differing_type == tile_type {
                enabling.push(EnablingBoundaryTile {
                    child_index: child_idx,
                    tile_type,
                    swap: swap.clone(),
                });
            }
        }
    }

    let geometric_min_distance = if !enabling.is_empty() { Some(1) } else { None };

    GeometricModificationAnalysis {
        swaps,
        boundary_tile_count,
        enabling_boundary_tiles: enabling,
        geometric_min_distance,
    }
}

/// Print the geometric modification distance analysis report.
pub fn print_geometric_modification_report(
    system: &dyn TilingSystem,
    analysis: &GeometricModificationAnalysis,
) {
    info!("=== Geometric Modification Distance Analysis ===\n");

    info!(
        "System: {} ({} boundary tiles, {} inflation children total)",
        system.name(),
        analysis.boundary_tile_count,
        system.num_inflation_children(),
    );

    if analysis.swaps.is_empty() {
        info!("No confusable pairs => geometric modification distance: INFINITE");
        return;
    }

    info!("\n--- Confusable pairs ---");
    for swap in &analysis.swaps {
        info!(
            "  {} ({} children) + 1 {} => {} ({} children)",
            system.supertile_name(swap.source_supertile),
            swap.source_size,
            system.type_name(swap.differing_type),
            system.supertile_name(swap.target_supertile),
            swap.source_size + 1,
        );
    }

    info!("\n--- Boundary tile analysis ---");
    if analysis.boundary_tile_count == 0 {
        info!("  No boundary tiles: all inflation children are owned by a supertile.");
        info!("  No tile can be at a shared boundary between adjacent supertiles.");
        info!("  => Geometric modification distance: INFINITE");
        return;
    }

    info!(
        "  {} boundary tile(s) not owned by any supertile.",
        analysis.boundary_tile_count
    );

    if analysis.enabling_boundary_tiles.is_empty() {
        let needed: Vec<_> = analysis
            .swaps
            .iter()
            .map(|s| system.type_name(s.differing_type))
            .collect();
        info!(
            "  None of the boundary tiles are type {} (required for a confusable-pair swap).",
            needed.join(" or ")
        );
        info!("  => Geometric modification distance: INFINITE");
    } else {
        for et in &analysis.enabling_boundary_tiles {
            info!(
                "  Child #{}: {} — enables {} ↔ {} swap",
                et.child_index,
                system.type_name(et.tile_type),
                system.supertile_name(et.swap.source_supertile),
                system.supertile_name(et.swap.target_supertile),
            );
        }
        info!("\n=== SUMMARY ===");
        info!("Geometric modification distance: 1");
        info!(
            "  {} boundary tile(s) of type {} exist.",
            analysis.enabling_boundary_tiles.len(),
            system.type_name(analysis.enabling_boundary_tiles[0].tile_type),
        );
        info!(
            "  Re-attributing one such tile from {} to {} (or vice versa) produces",
            system.supertile_name(analysis.enabling_boundary_tiles[0].swap.source_supertile),
            system.supertile_name(analysis.enabling_boundary_tiles[0].swap.target_supertile),
        );
        info!("  a geometrically distinct but locally valid hierarchy with 1 boundary change.");
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::metatile::MetatileType;
    use crate::oneway::build_flat_hierarchy;

    #[test]
    fn only_pf_swap_exists() {
        let system = HatSystem::new();
        let swaps = enumerate_valid_swaps(&system);
        assert!(
            !swaps.is_empty(),
            "should find at least one valid swap"
        );

        let swap_pairs: HashSet<(usize, usize)> = swaps
            .iter()
            .map(|s| {
                if s.source_supertile < s.target_supertile {
                    (s.source_supertile, s.target_supertile)
                } else {
                    (s.target_supertile, s.source_supertile)
                }
            })
            .collect();

        assert_eq!(
            swap_pairs.len(),
            1,
            "should have exactly 1 unique swap pair, got: {:?}",
            swap_pairs
        );
        // P=2, F=3 in hat system
        assert!(
            swap_pairs.contains(&(MetatileType::P.index(), MetatileType::F.index())),
            "the only swap pair should be P<->F, got: {:?}",
            swap_pairs
        );
    }

    #[test]
    fn h_supertile_has_critical_position() {
        let system = HatSystem::new();
        let crit = compute_position_criticality(&system);
        let h_positions: Vec<&PositionCriticality> = crit
            .iter()
            .filter(|p| p.supertile == MetatileType::H.index())
            .collect();

        assert_eq!(h_positions.len(), 10, "H' should have 10 child positions");

        let has_critical = h_positions.iter().any(|p| p.score == 1);
        let has_redundant = h_positions.iter().any(|p| p.score == 0);
        assert!(
            has_critical || has_redundant,
            "H' should have at least some critical or redundant positions"
        );
    }

    #[test]
    fn t_supertile_single_child_is_critical() {
        let system = HatSystem::new();
        let crit = compute_position_criticality(&system);
        let t_positions: Vec<&PositionCriticality> = crit
            .iter()
            .filter(|p| p.supertile == MetatileType::T.index())
            .collect();

        assert_eq!(t_positions.len(), 1, "T' should have 1 child position");
        assert_eq!(
            t_positions[0].score, 0,
            "T' single child should be redundant (T' is uniquely small)"
        );
    }

    #[test]
    fn minimum_determining_set_sizes() {
        let system = HatSystem::new();
        let sets = minimum_determining_sets(&system);
        assert_eq!(sets.len(), 4, "should have one determining set per supertile type");

        for ds in &sets {
            let max_size = system.supertile_children(ds.supertile).len();
            assert!(
                ds.size <= max_size,
                "{} determining set size {} > child count {}",
                system.supertile_name(ds.supertile),
                ds.size,
                max_size
            );
        }

        let t_set = sets
            .iter()
            .find(|d| d.supertile == MetatileType::T.index())
            .unwrap();
        assert!(t_set.size <= 1, "T' should need at most 1 position");
    }

    #[test]
    fn erasure_at_zero_is_fully_recoverable() {
        let hierarchy = build_flat_hierarchy(MetatileType::H, 2);
        let adj = full_sibling_adjacency(&hierarchy, 0);
        let results = erasure_sweep(&hierarchy, 0, &adj, &[0.0], 1);

        assert_eq!(results.len(), 1);
        assert!(
            (results[0].mean_recoverable - 1.0).abs() < 1e-10,
            "0% erasure should mean 100% surviving, got {}",
            results[0].mean_recoverable
        );
        assert!(
            (results[0].mean_determined - 1.0).abs() < 1e-10,
            "0% erasure should mean 100% determined, got {}",
            results[0].mean_determined
        );
    }

    #[test]
    fn erasure_at_one_is_unrecoverable() {
        let hierarchy = build_flat_hierarchy(MetatileType::H, 2);
        let adj = full_sibling_adjacency(&hierarchy, 0);
        let results = erasure_sweep(&hierarchy, 0, &adj, &[1.0], 1);

        assert_eq!(results.len(), 1);
        assert!(
            results[0].mean_recoverable < 0.01,
            "100% erasure should leave ~0% surviving, got {}",
            results[0].mean_recoverable
        );
    }

    #[test]
    fn dependency_graph_structure() {
        let hierarchy = build_flat_hierarchy(MetatileType::H, 2);
        let adj = full_sibling_adjacency(&hierarchy, 0);
        let deps = build_dependency_graph(&hierarchy, 0, &adj, 1);

        assert_eq!(
            deps.len(),
            hierarchy.tile_types[0].len(),
            "dependency graph should have one entry per tile"
        );

        let n = deps.len();
        for (tile, targets) in deps.iter().enumerate() {
            for &target in targets {
                assert!(
                    target < n,
                    "tile {} depends on out-of-range tile {}",
                    tile,
                    target
                );
            }
        }
    }

    #[test]
    fn criticality_scores_sum_correctly() {
        let system = HatSystem::new();
        let crit = compute_position_criticality(&system);

        let expected_total: usize = (0..system.num_types())
            .map(|i| system.supertile_children(i).len())
            .sum();
        assert_eq!(
            crit.len(),
            expected_total,
            "should have one criticality entry per child position across all supertile types"
        );

        for p in &crit {
            assert!(
                p.score <= 1,
                "criticality score should be 0 or 1, got {}",
                p.score
            );
        }
    }
}
